{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard python packages\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../scripts\")\n",
    "\n",
    "path = os.path.join('../data', 'processed', 'ner - locatia accidente_clean.csv')\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Deviding the Training-Set from the testing set\n",
    "\n",
    "# join half of the real phrases with half of the generated ones for both training and testing\n",
    "records_train = df[['text_no_sw_no_bars','Entities_position']].iloc[np.r_[0:80, 160:900]].to_records(index=False)\n",
    "records_test = df[['text_no_sw_no_bars','Entities_position']].iloc[np.r_[80:156, 900:1450]].to_records(index=False)\n",
    "\n",
    "records_test_real = df[['text_no_sw_no_bars','Entities_position']][80:156].to_records(index=False)\n",
    "records_test_generated = df[['text_no_sw_no_bars','Entities_position']][900:1450].to_records(index=False)\n",
    "\n",
    "\n",
    "result_train = list(records_train)\n",
    "result_test = list(records_test)\n",
    "\n",
    "result_test_real = list(records_test_real)\n",
    "result_test_generated = list(records_test_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 820  training samples\n",
      "We have 626  testing samples\n",
      "We have 76   real testing samples\n",
      "We have 626  generated testing samples\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-40f0dbbe1de1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtesting_data_generated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_test_generated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"We have\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" generated testing samples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "training_data = result_train\n",
    "print(\"We have\",len(training_data),\" training samples\")\n",
    "\n",
    "testing_data = result_test\n",
    "print(\"We have\",len(testing_data),\" testing samples\")\n",
    "\n",
    "testing_data_real = result_test_real\n",
    "print(\"We have\",len(testing_data_real),\"  real testing samples\")\n",
    "\n",
    "testing_data_generated = result_test_generated\n",
    "print(\"We have\",len(testing_data),\" generated testing samples\")\n",
    "print(testing_data[0][0]['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.evaluate import check_accuracy\n",
    "        \n",
    "nlp_model = spacy.load('../src/models/')\n",
    "\n",
    "def remove_whitespace_entities(doc):\n",
    "    doc.ents = [e for e in doc.ents if not e.text.isspace()]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(remove_whitespace_entities, after='ner')\n",
    "\n",
    "# doc = nlp(\"Totul s-a întâmplat în data de 27 octombrie pe o stradă din municipiul hincesti\")\n",
    "# doc = nlp(\"O şoferiţă din Chişinău a intrat dur cu al său Mercedes într-o staţie de aşteptare a transportului public Accidentul cu pricina a avut loc în această după amiază în apropriere de intersecţia străzilor Meșterul Manole și Vadul lui Vodă din sectorul Ciocana\")\n",
    "doc = nlp('De unde şi când a apărut acolo? În această după amiază, pe strada Nicolae Dimo din capitală, camera de bord dintr-un SUV a surprins momentul în care automobilul premergător intră brusc într-un obstacol. Apariţia în trafic s-a dovedit a fi un alt autovehicul care se afla pe contrasens!')\n",
    "\n",
    "\n",
    "list_ent_results = []\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "    tuple_ent =(ent.start_char, ent.end_char, ent.label_) \n",
    "    list_ent_results.append(tuple_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.evaluate import check_accuracy\n",
    "\n",
    "## Get accuracy by testing the whole testing_set(real + generated)\n",
    "dict_with_phrases, dict_false_positive, dict_false_negative =  check_accuracy(test_list=testing_data, model=nlp, test_set_name='All')\n",
    "\n",
    "## Get accuracy by testing the whole real_testing_set(real only)\n",
    "dict_with_phrases, dict_false_positive, dict_false_negative =  check_accuracy(testing_data_real, model=nlp, test_set_name='Real_data')\n",
    "\n",
    "## Get accuracy by testing the whole generated_testing_set(generated only)\n",
    "dict_with_phrases, dict_false_positive, dict_false_negative = check_accuracy(testing_data_generated, model=nlp, test_set_name='Generated_data')\n",
    "\n",
    "\n",
    "##Create dict that holds all the wrong predictions made by the model\n",
    "df2 = pd.DataFrame()\n",
    "df2['PHRASE_WRONG'] = pd.Series(dict_with_phrases)\n",
    "df2['Location_false_positive'] = pd.Series(dict_false_positive)\n",
    "df2['Location_false_negative'] = pd.Series(dict_false_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT POSITIONS ONLY || [(112, 120, LOC_ACCIDENT)] --> [112, 120] || FROM THE LISTS\n",
    "from nlp.evaluate import extract_pos\n",
    "\n",
    "if 'false_positive_poss' in df2.columns:\n",
    "    df2 = df2.drop(columns='false_positive_poss')\n",
    "if 'false_negative_poss' in df2.columns:\n",
    "    df2 = df2.drop(columns='false_negative_poss')\n",
    "\n",
    "df2['false_positive_poss'] = df2['Location_false_positive'].apply(extract_pos)\n",
    "df2['false_negative_poss'] = df2['Location_false_negative'].apply(extract_pos)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.evaluate import extract_word\n",
    "\n",
    "\n",
    "if 'false_positive_words' in df2.columns:\n",
    "    df2 = df2.drop(columns='false_positive_words')\n",
    "if 'false_negative_words' in df2.columns:\n",
    "    df2 = df2.drop(columns='false_negative_words')\n",
    "\n",
    "# axis = 0: by column = column-wise = along the rows\n",
    "# axis = 1: by row = row-wise = along the columns\n",
    "df2[\"false_positive_words\"] = df2[[\"PHRASE_WRONG\", \"false_positive_poss\"]].apply(extract_word, axis=1)\n",
    "df2[\"false_negative_words\"] = df2[[\"PHRASE_WRONG\", \"false_negative_poss\"]].apply(extract_word, axis=1)\n",
    "\n",
    "if 'false_positive_words_string' in df2.columns:\n",
    "    df2 = df2.drop(columns='false_positive_words_string')\n",
    "if 'false_negative_words_string' in df2.columns:\n",
    "    df2 = df2.drop(columns='false_negative_words_string')\n",
    "\n",
    "df2['false_positive_words_string'] = df2['false_positive_words'].apply(lambda x: ' '.join(x))\n",
    "df2['false_negative_words_string'] = df2['false_negative_words'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# pd.set_option('display.max_rows', 100) \n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_entities_in_dataframe(dataframe_column = df2['PHRASE_WRONG'],model_path = '../src/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false_positive_words = []\n",
    "for phrases in df2[\"false_positive_words_string\"]:\n",
    "    if len(phrases) < 1:\n",
    "        pass\n",
    "    else:\n",
    "        all_false_positive_words.append(phrases)\n",
    "    \n",
    "generate_cloud(df2[\"false_positive_words_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_false_positive_words = []\n",
    "for phrases in df2[\"false_negative_words_string\"]:\n",
    "    if len(phrases) < 1:\n",
    "        pass\n",
    "    else:\n",
    "        all_false_positive_words.append(phrases)\n",
    "    \n",
    "generate_cloud(df2[\"false_negative_words_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the model metrics\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png \"Title\")\n",
    "\n",
    "UAS (Unlabelled Attachment Score) and LAS (Labelled Attachment Score) are standard metrics to evaluate dependency parsing. UAS is the proportion of tokens whose head has been correctly assigned, LAS is the proportion of tokens whose head has been correctly assigned with the right dependency label (subject, object, etc).\n",
    "\n",
    "ents_p = precision ||\n",
    "ents_r = recall ||\n",
    "ents_f = fscore ||\n",
    "for the NER task\n",
    "tags_acc is the POS tagging accuracy.\n",
    "token_acc is precision for token segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# • recall\n",
    "# • presicion\n",
    "# • confusion matrix\n",
    "# • example paterns in false positives\n",
    "# • example paterns in false negatives\n",
    "\n",
    "from nlp.evaluate import evaluate\n",
    "\n",
    "\n",
    "\n",
    "all_examples = testing_data\n",
    "real_examples = testing_data_real\n",
    "generated_examples = testing_data_generated\n",
    "\n",
    "results = evaluate(nlp, all_examples)\n",
    "print(f\"--------------------------\\nFor all_examples\\nprecision is: {results['ents_p']}\\nrecall is: {results['ents_r']}\\nfscore is: {results['ents_f']}\")\n",
    "\n",
    "results = evaluate(nlp, real_examples)\n",
    "print(f\"-------------------------\\nFor real_examples\\nprecision is: {results['ents_p']}\\nrecall is: {results['ents_r']}\\nfscore is: {results['ents_f']}\")\n",
    "\n",
    "results = evaluate(nlp, generated_examples)\n",
    "print(f\"-------------------------\\nFor generated_examples\\nprecision is: {results['ents_p']}\\nrecall is: {results['ents_r']}\\nfscore is: {results['ents_f']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
